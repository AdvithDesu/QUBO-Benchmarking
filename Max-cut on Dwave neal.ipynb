{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27a2c1d-57c9-4de5-abd7-9f945894dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# D-Wave Ocean SDK imports\n",
    "import dimod\n",
    "from dwave.samplers import SimulatedAnnealingSampler, TabuSampler\n",
    "from neal import SimulatedAnnealingSampler as NealSampler\n",
    "try:\n",
    "    from dwave.system import DWaveSampler, EmbeddingComposite\n",
    "    DWAVE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"D-Wave system not available - using simulators only\")\n",
    "    DWAVE_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c8e3e3-b47f-4953-96a6-6f2fc4a59729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GsetLoader:\n",
    "    \"\"\"Load and process Gset Max-Cut benchmark instances\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"gset_data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self.base_url = \"http://web.stanford.edu/~yyye/yyye/Gset/\"\n",
    "        \n",
    "        # Gset instance information\n",
    "        self.instance_info = {\n",
    "            # Small instances good for D-Wave\n",
    "            11: {\"nodes\": 800, \"edges\": 1600, \"density\": 0.005, \"best_known\": 564},\n",
    "            12: {\"nodes\": 800, \"edges\": 1600, \"density\": 0.005, \"best_known\": 556},\n",
    "            13: {\"nodes\": 800, \"edges\": 1600, \"density\": 0.005, \"best_known\": 582},\n",
    "            14: {\"nodes\": 800, \"edges\": 4694, \"density\": 0.015, \"best_known\": 3064},\n",
    "            \n",
    "            # Medium instances\n",
    "            22: {\"nodes\": 2000, \"edges\": 19990, \"density\": 0.010, \"best_known\": 13359},\n",
    "            43: {\"nodes\": 1000, \"edges\": 9990, \"density\": 0.020, \"best_known\": 6660},\n",
    "            44: {\"nodes\": 1000, \"edges\": 9990, \"density\": 0.020, \"best_known\": 6650},\n",
    "            \n",
    "            # Large instances (GPU territory)\n",
    "            48: {\"nodes\": 3000, \"edges\": 6000, \"density\": 0.001, \"best_known\": 6000},\n",
    "            49: {\"nodes\": 3000, \"edges\": 6000, \"density\": 0.001, \"best_known\": 6000},\n",
    "            54: {\"nodes\": 5000, \"edges\": 12498, \"density\": 0.001, \"best_known\": 4030},\n",
    "            55: {\"nodes\": 5000, \"edges\": 12498, \"density\": 0.001, \"best_known\": 10294},\n",
    "            \n",
    "            # Very large instances\n",
    "            70: {\"nodes\": 10000, \"edges\": 9999, \"density\": 0.0002, \"best_known\": 9541},\n",
    "            71: {\"nodes\": 10000, \"edges\": 9999, \"density\": 0.0002, \"best_known\": 9552},\n",
    "            72: {\"nodes\": 10000, \"edges\": 9999, \"density\": 0.0002, \"best_known\": 6037},\n",
    "            81: {\"nodes\": 20000, \"edges\": 40000, \"density\": 0.0002, \"best_known\": 15364}\n",
    "        }\n",
    "    \n",
    "    def download_instance(self, instance_num: int) -> Optional[Path]:\n",
    "        \"\"\"Download Gset instance if not already cached\"\"\"\n",
    "        filename = f\"G{instance_num}\"\n",
    "        filepath = self.data_dir / filename\n",
    "        \n",
    "        if filepath.exists():\n",
    "            return filepath\n",
    "        \n",
    "        url = f\"{self.base_url}{filename}\"\n",
    "        print(f\"Downloading G{instance_num}...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(response.text)\n",
    "            \n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading G{instance_num}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_instance(self, instance_num: int) -> Optional[nx.Graph]:\n",
    "        \"\"\"Load Gset instance as NetworkX graph\"\"\"\n",
    "        filepath = self.download_instance(instance_num)\n",
    "        if not filepath:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Parse header\n",
    "            first_line = lines[0].strip().split()\n",
    "            num_vertices = int(first_line[0])\n",
    "            num_edges = int(first_line[1])\n",
    "            \n",
    "            # Create graph with 0-based indexing\n",
    "            G = nx.Graph()\n",
    "            G.add_nodes_from(range(num_vertices))\n",
    "            \n",
    "            # Add edges\n",
    "            for line in lines[1:]:\n",
    "                if line.strip():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 3:\n",
    "                        # Convert to 0-based indexing\n",
    "                        u, v, weight = int(parts[0])-1, int(parts[1])-1, float(parts[2])\n",
    "                        G.add_edge(u, v, weight=weight)\n",
    "            \n",
    "            return G\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing G{instance_num}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9bfe8c1-2d99-41bc-9898-73cb76683fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxCutQUBO:\n",
    "    \"\"\"Max-Cut QUBO formulation and utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def graph_to_qubo(G: nx.Graph) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Convert Max-Cut problem to QUBO formulation\"\"\"\n",
    "        Q = {}\n",
    "        \n",
    "        # Max-Cut QUBO: minimize -0.5 * sum(w_ij * (x_i - x_j)^2)\n",
    "        # Expanded: minimize -0.5 * sum(w_ij * (x_i + x_j - 2*x_i*x_j))\n",
    "        # Which gives: minimize sum(w_ij * (x_i*x_j - 0.5*(x_i + x_j)))\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            weight = data.get('weight', 1.0)\n",
    "            \n",
    "            # Diagonal terms: +0.5 * w_ij for each vertex\n",
    "            Q[(u, u)] = Q.get((u, u), 0) + 0.5 * weight\n",
    "            Q[(v, v)] = Q.get((v, v), 0) + 0.5 * weight\n",
    "            \n",
    "            # Off-diagonal term: -w_ij\n",
    "            Q[(u, v)] = Q.get((u, v), 0) - weight\n",
    "        \n",
    "        return Q\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_cut(G: nx.Graph, solution: Dict[int, int]) -> float:\n",
    "        \"\"\"Evaluate Max-Cut objective for a solution\"\"\"\n",
    "        cut_value = 0\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            weight = data.get('weight', 1.0)\n",
    "            if solution[u] != solution[v]:  # Edge crosses the cut\n",
    "                cut_value += weight\n",
    "        return cut_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def qubo_energy_to_cut_value(qubo_energy: float, total_weight: float) -> float:\n",
    "        \"\"\"Convert QUBO energy back to Max-Cut value\"\"\"\n",
    "        # QUBO energy = 0.5 * total_weight - cut_value\n",
    "        return 0.5 * total_weight - qubo_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a83e177-dae0-4aa5-93d0-a22eca202054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxCutBenchmark:\n",
    "    \"\"\"Comprehensive Max-Cut benchmarking system\"\"\"\n",
    "    \n",
    "    def __init__(self, results_dir=\"maxcut_results\"):\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.loader = GsetLoader()\n",
    "        self.results = {}\n",
    "        \n",
    "        # Initialize samplers\n",
    "        self.samplers = self._initialize_samplers()\n",
    "        \n",
    "    def _initialize_samplers(self) -> Dict:\n",
    "        \"\"\"Initialize all available samplers\"\"\"\n",
    "        samplers = {\n",
    "            'neal': NealSampler(),\n",
    "            'simulated_annealing': SimulatedAnnealingSampler(),\n",
    "            'tabu': TabuSampler()\n",
    "        }\n",
    "        \n",
    "        # Add D-Wave if available\n",
    "        if DWAVE_AVAILABLE:\n",
    "            try:\n",
    "                samplers['dwave'] = EmbeddingComposite(DWaveSampler())\n",
    "                print(\"D-Wave sampler initialized successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"D-Wave sampler initialization failed: {e}\")\n",
    "        \n",
    "        print(f\"Initialized samplers: {list(samplers.keys())}\")\n",
    "        return samplers\n",
    "    \n",
    "    def benchmark_instance(self, instance_num: int, num_runs: int = 5) -> Dict:\n",
    "        \"\"\"Benchmark single Gset instance across all samplers\"\"\"\n",
    "        print(f\"\\n=== Benchmarking G{instance_num} ===\")\n",
    "        \n",
    "        # Load instance\n",
    "        G = self.loader.load_instance(instance_num)\n",
    "        if not G:\n",
    "            print(f\"Failed to load G{instance_num}\")\n",
    "            return {}\n",
    "        \n",
    "        Q = MaxCutQUBO.graph_to_qubo(G)\n",
    "        instance_info = self.loader.instance_info.get(instance_num, {})\n",
    "        best_known = instance_info.get('best_known', None)\n",
    "        \n",
    "        # Calculate total edge weight for energy conversion\n",
    "        total_weight = sum(data.get('weight', 1.0) for _, _, data in G.edges(data=True))\n",
    "        \n",
    "        print(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "        print(f\"QUBO terms: {len(Q)}, Best known: {best_known}\")\n",
    "        \n",
    "        results = {\n",
    "            'instance': instance_num,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'density': instance_info.get('density', 0),\n",
    "            'best_known': best_known,\n",
    "            'total_weight': total_weight,\n",
    "            'samplers': {}\n",
    "        }\n",
    "        \n",
    "        # Test each sampler\n",
    "        for sampler_name, sampler in self.samplers.items():\n",
    "            print(f\"\\nTesting {sampler_name}...\")\n",
    "            \n",
    "            sampler_results = {\n",
    "                'energies': [],\n",
    "                'cut_values': [],\n",
    "                'times': [],\n",
    "                'successful_runs': 0\n",
    "            }\n",
    "            \n",
    "            for run in range(num_runs):\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Configure sampler parameters\n",
    "                    if sampler_name == 'dwave':\n",
    "                        response = sampler.sample_qubo(\n",
    "                            Q, \n",
    "                            num_reads=1000,\n",
    "                            annealing_time=20,\n",
    "                            chain_strength=max(abs(v) for v in Q.values()) * 2\n",
    "                        )\n",
    "                    elif sampler_name == 'neal':\n",
    "                        response = sampler.sample_qubo(\n",
    "                            Q, \n",
    "                            num_reads=1000,\n",
    "                            num_sweeps=10000\n",
    "                        )\n",
    "                    elif sampler_name == 'tabu':\n",
    "                        response = sampler.sample_qubo(Q, num_reads=10, timeout=60)\n",
    "                    else:  # simulated_annealing\n",
    "                        response = sampler.sample_qubo(Q, num_reads=100)\n",
    "                    \n",
    "                    run_time = time.time() - start_time\n",
    "                    \n",
    "                    # Get best solution\n",
    "                    best_sample = response.first\n",
    "                    energy = best_sample.energy\n",
    "                    solution = dict(best_sample.sample)\n",
    "                    \n",
    "                    # Calculate cut value\n",
    "                    cut_value = MaxCutQUBO.evaluate_cut(G, solution)\n",
    "                    \n",
    "                    sampler_results['energies'].append(energy)\n",
    "                    sampler_results['cut_values'].append(cut_value)\n",
    "                    sampler_results['times'].append(run_time)\n",
    "                    sampler_results['successful_runs'] += 1\n",
    "                    \n",
    "                    print(f\"  Run {run+1}: Cut={cut_value:.1f}, Time={run_time:.2f}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Run {run+1} failed: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Calculate statistics\n",
    "            if sampler_results['successful_runs'] > 0:\n",
    "                sampler_results['best_cut'] = max(sampler_results['cut_values'])\n",
    "                sampler_results['avg_cut'] = np.mean(sampler_results['cut_values'])\n",
    "                sampler_results['std_cut'] = np.std(sampler_results['cut_values'])\n",
    "                sampler_results['avg_time'] = np.mean(sampler_results['times'])\n",
    "                sampler_results['std_time'] = np.std(sampler_results['times'])\n",
    "                \n",
    "                # Calculate approximation ratio if best known available\n",
    "                if best_known:\n",
    "                    sampler_results['approximation_ratio'] = sampler_results['best_cut'] / best_known\n",
    "                    sampler_results['avg_approximation_ratio'] = sampler_results['avg_cut'] / best_known\n",
    "                \n",
    "                print(f\"  Best cut: {sampler_results['best_cut']:.1f}\")\n",
    "                if best_known:\n",
    "                    print(f\"  Approximation ratio: {sampler_results['approximation_ratio']:.4f}\")\n",
    "            \n",
    "            results['samplers'][sampler_name] = sampler_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_systematic_benchmark(self, instance_list: List[int] = None, num_runs: int = 5):\n",
    "        \"\"\"Run systematic benchmark across instance progression\"\"\"\n",
    "        if instance_list is None:\n",
    "            # Default progression from small to large\n",
    "            instance_list = [11, 12, 13, 14, 43]#, 44, 48, 49, 54, 70, 71, 81]\n",
    "        \n",
    "        print(f\"Starting systematic benchmark of {len(instance_list)} instances\")\n",
    "        print(f\"Instance progression: {instance_list}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for instance_num in instance_list:\n",
    "            try:\n",
    "                result = self.benchmark_instance(instance_num, num_runs)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "                    self.results[instance_num] = result\n",
    "                    \n",
    "                    # Save intermediate results\n",
    "                    self._save_results()\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nBenchmark interrupted by user\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error benchmarking G{instance_num}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Generate final report\n",
    "        self._generate_report(all_results)\n",
    "        return all_results\n",
    "    \n",
    "    def _save_results(self):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        results_file = self.results_dir / \"benchmark_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "    \n",
    "    def analyze_key_metrics(self, results: List[Dict]):\n",
    "        \"\"\"Analyze and report key performance metrics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KEY METRIC ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        # Extract data for analysis\n",
    "        sampler_names = list(results[0]['samplers'].keys())\n",
    "        \n",
    "        print(\"\\n1. SOLUTION QUALITY ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        quality_summary = {}\n",
    "        \n",
    "        for sampler_name in sampler_names:\n",
    "            ratios = []\n",
    "            for result in results:\n",
    "                if (result['best_known'] and \n",
    "                    result['samplers'][sampler_name]['successful_runs'] > 0):\n",
    "                    ratio = result['samplers'][sampler_name]['best_cut'] / result['best_known']\n",
    "                    ratios.append(ratio)\n",
    "            \n",
    "            if ratios:\n",
    "                quality_summary[sampler_name] = {\n",
    "                    'avg_ratio': np.mean(ratios),\n",
    "                    'min_ratio': min(ratios),\n",
    "                    'max_ratio': max(ratios),\n",
    "                    'std_ratio': np.std(ratios)\n",
    "                }\n",
    "                \n",
    "                print(f\"{sampler_name:20}: Avg={quality_summary[sampler_name]['avg_ratio']:.4f}, \"\n",
    "                      f\"Range=[{quality_summary[sampler_name]['min_ratio']:.4f}-\"\n",
    "                      f\"{quality_summary[sampler_name]['max_ratio']:.4f}], \"\n",
    "                      f\"Std={quality_summary[sampler_name]['std_ratio']:.4f}\")\n",
    "        \n",
    "        print(\"\\n2. RUNTIME PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        runtime_summary = {}\n",
    "        \n",
    "        for sampler_name in sampler_names:\n",
    "            times = []\n",
    "            nodes = []\n",
    "            for result in results:\n",
    "                if result['samplers'][sampler_name]['successful_runs'] > 0:\n",
    "                    times.append(result['samplers'][sampler_name]['avg_time'])\n",
    "                    nodes.append(result['nodes'])\n",
    "            \n",
    "            if times:\n",
    "                runtime_summary[sampler_name] = {\n",
    "                    'avg_time': np.mean(times),\n",
    "                    'min_time': min(times),\n",
    "                    'max_time': max(times),\n",
    "                    'time_per_node': np.mean([t/n for t, n in zip(times, nodes)])\n",
    "                }\n",
    "                \n",
    "                print(f\"{sampler_name:20}: Avg={runtime_summary[sampler_name]['avg_time']:.2f}s, \"\n",
    "                      f\"Range=[{runtime_summary[sampler_name]['min_time']:.2f}-\"\n",
    "                      f\"{runtime_summary[sampler_name]['max_time']:.2f}]s, \"\n",
    "                      f\"Time/Node={runtime_summary[sampler_name]['time_per_node']*1000:.2f}ms\")\n",
    "        \n",
    "        print(\"\\n3. SCALABILITY ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Analyze largest problem each sampler could handle\n",
    "        max_nodes_handled = {}\n",
    "        for sampler_name in sampler_names:\n",
    "            max_nodes = 0\n",
    "            for result in results:\n",
    "                if result['samplers'][sampler_name]['successful_runs'] > 0:\n",
    "                    max_nodes = max(max_nodes, result['nodes'])\n",
    "            max_nodes_handled[sampler_name] = max_nodes\n",
    "            print(f\"{sampler_name:20}: Max problem size = {max_nodes} nodes\")\n",
    "        \n",
    "        print(\"\\n4. EFFICIENCY RANKING\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Combine quality and speed for efficiency ranking\n",
    "        efficiency_scores = {}\n",
    "        for sampler_name in sampler_names:\n",
    "            if (sampler_name in quality_summary and \n",
    "                sampler_name in runtime_summary):\n",
    "                \n",
    "                # Higher quality ratio = better, lower time = better\n",
    "                quality_score = quality_summary[sampler_name]['avg_ratio']\n",
    "                speed_score = 1.0 / runtime_summary[sampler_name]['avg_time']  # Invert so higher is better\n",
    "                \n",
    "                # Normalize and combine (you can adjust weights)\n",
    "                quality_weight = 0.6  # Prioritize solution quality\n",
    "                speed_weight = 0.4\n",
    "                \n",
    "                efficiency_scores[sampler_name] = (\n",
    "                    quality_weight * quality_score + \n",
    "                    speed_weight * speed_score / max(s for s in [speed_score] + \n",
    "                                                   [1.0/runtime_summary[s]['avg_time'] \n",
    "                                                    for s in runtime_summary])\n",
    "                )\n",
    "        \n",
    "        # Sort by efficiency\n",
    "        ranked_samplers = sorted(efficiency_scores.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"Overall Efficiency Ranking (Quality=60%, Speed=40%):\")\n",
    "        for i, (sampler_name, score) in enumerate(ranked_samplers):\n",
    "            print(f\"{i+1}. {sampler_name:20}: Efficiency Score = {score:.4f}\")\n",
    "        \n",
    "        print(\"\\n5. RESEARCH RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'dwave' in quality_summary and 'neal' in quality_summary:\n",
    "            dwave_quality = quality_summary['dwave']['avg_ratio']\n",
    "            neal_quality = quality_summary['neal']['avg_ratio']\n",
    "            quality_diff = abs(dwave_quality - neal_quality)\n",
    "            \n",
    "            if quality_diff < 0.01:\n",
    "                print(\"• Solution quality is comparable between D-Wave and GPU simulators\")\n",
    "            elif dwave_quality > neal_quality:\n",
    "                print(f\"• D-Wave shows {((dwave_quality/neal_quality-1)*100):.1f}% better solution quality\")\n",
    "            else:\n",
    "                print(f\"• GPU simulator shows {((neal_quality/dwave_quality-1)*100):.1f}% better solution quality\")\n",
    "        \n",
    "        if 'dwave' in runtime_summary and 'neal' in runtime_summary:\n",
    "            dwave_time = runtime_summary['dwave']['avg_time']\n",
    "            neal_time = runtime_summary['neal']['avg_time']\n",
    "            \n",
    "            if dwave_time > neal_time:\n",
    "                print(f\"• GPU simulator is {(dwave_time/neal_time):.1f}x faster than D-Wave\")\n",
    "            else:\n",
    "                print(f\"• D-Wave is {(neal_time/dwave_time):.1f}x faster than GPU simulator\")\n",
    "        \n",
    "        # Identify crossover point\n",
    "        dwave_max = max_nodes_handled.get('dwave', 0)\n",
    "        gpu_max = max(max_nodes_handled.get(s, 0) for s in max_nodes_handled if s != 'dwave')\n",
    "        \n",
    "        if dwave_max > 0 and gpu_max > dwave_max:\n",
    "            print(f\"• Quantum advantage disappears around {dwave_max} nodes\")\n",
    "            print(f\"• GPU simulators scale to {gpu_max}+ nodes\")\n",
    "        \n",
    "        return quality_summary, runtime_summary, efficiency_scores\n",
    "    def _generate_report(self, results: List[Dict]):\n",
    "        \"\"\"Generate comprehensive benchmark report\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        # First, analyze key metrics\n",
    "        self.analyze_key_metrics(results)\n",
    "        \n",
    "        # Create summary table\n",
    "        summary_data = []\n",
    "        for result in results:\n",
    "            instance = result['instance']\n",
    "            nodes = result['nodes']\n",
    "            best_known = result['best_known']\n",
    "            \n",
    "            row = {\n",
    "                'Instance': f\"G{instance}\",\n",
    "                'Nodes': nodes,\n",
    "                'Edges': result['edges'],\n",
    "                'Density': f\"{result['density']:.4f}\",\n",
    "                'Best_Known': best_known if best_known else 'Unknown'\n",
    "            }\n",
    "            \n",
    "            # Add sampler results\n",
    "            for sampler_name, sampler_result in result['samplers'].items():\n",
    "                if sampler_result['successful_runs'] > 0:\n",
    "                    row[f'{sampler_name}_best'] = f\"{sampler_result['best_cut']:.1f}\"\n",
    "                    row[f'{sampler_name}_time'] = f\"{sampler_result['avg_time']:.2f}s\"\n",
    "                    if best_known:\n",
    "                        ratio = sampler_result['best_cut'] / best_known\n",
    "                        row[f'{sampler_name}_ratio'] = f\"{ratio:.4f}\"\n",
    "                else:\n",
    "                    row[f'{sampler_name}_best'] = 'Failed'\n",
    "                    row[f'{sampler_name}_time'] = 'N/A'\n",
    "                    row[f'{sampler_name}_ratio'] = 'N/A'\n",
    "            \n",
    "            summary_data.append(row)\n",
    "        \n",
    "        # Save summary table\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        csv_file = self.results_dir / \"benchmark_summary.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"\\nSummary saved to: {csv_file}\")\n",
    "        print(\"\\nBenchmark Summary:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Create visualizations\n",
    "        self._create_visualizations(results)\n",
    "    \n",
    "    def _create_visualizations(self, results: List[Dict]):\n",
    "        \"\"\"Create benchmark visualization plots\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        instances = [r['instance'] for r in results]\n",
    "        nodes = [r['nodes'] for r in results]\n",
    "        \n",
    "        # Plot 1: Solution quality comparison (PRIMARY METRIC)\n",
    "        sampler_names = list(results[0]['samplers'].keys())\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(sampler_names)))\n",
    "        \n",
    "        for i, sampler_name in enumerate(sampler_names):\n",
    "            ratios = []\n",
    "            valid_instances = []\n",
    "            for result in results:\n",
    "                if result['best_known'] and result['samplers'][sampler_name]['successful_runs'] > 0:\n",
    "                    ratio = result['samplers'][sampler_name]['best_cut'] / result['best_known']\n",
    "                    ratios.append(ratio)\n",
    "                    valid_instances.append(result['instance'])\n",
    "            \n",
    "            if ratios:\n",
    "                ax1.plot(valid_instances, ratios, 'o-', label=sampler_name, \n",
    "                        color=colors[i], markersize=6)\n",
    "        \n",
    "        ax1.set_xlabel('Instance Number (Problem Size →)')\n",
    "        ax1.set_ylabel('Approximation Ratio (Quality)')\n",
    "        ax1.set_title('QUALITY: How Close to Optimal? (Higher = Better)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Perfect (Optimal)')\n",
    "        ax1.set_ylim(0.85, 1.05)\n",
    "        \n",
    "        # Plot 2: Runtime scaling (PRIMARY METRIC)\n",
    "        for i, sampler_name in enumerate(sampler_names):\n",
    "            times = []\n",
    "            valid_nodes = []\n",
    "            for result in results:\n",
    "                if result['samplers'][sampler_name]['successful_runs'] > 0:\n",
    "                    times.append(result['samplers'][sampler_name]['avg_time'])\n",
    "                    valid_nodes.append(result['nodes'])\n",
    "            \n",
    "            if times:\n",
    "                ax2.loglog(valid_nodes, times, 'o-', label=sampler_name, \n",
    "                          color=colors[i], markersize=6, linewidth=2)\n",
    "        \n",
    "        ax2.set_xlabel('Problem Size (Number of Nodes)')\n",
    "        ax2.set_ylabel('Runtime (seconds) - Log Scale')\n",
    "        ax2.set_title('SPEED: Runtime Scaling (Lower = Faster)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add reference lines for different scaling behaviors\n",
    "        if valid_nodes:\n",
    "            min_nodes, max_nodes = min(valid_nodes), max(valid_nodes)\n",
    "            x_ref = np.array([min_nodes, max_nodes])\n",
    "            ax2.plot(x_ref, x_ref/min_nodes * 0.1, '--', alpha=0.5, label='Linear O(n)', color='gray')\n",
    "            ax2.plot(x_ref, (x_ref/min_nodes)**2 * 0.01, '--', alpha=0.5, label='Quadratic O(n²)', color='gray')\n",
    "        \n",
    "        # Plot 3: Success rate by instance size\n",
    "        size_bins = [0, 1000, 3000, 10000, float('inf')]\n",
    "        size_labels = ['<1K', '1K-3K', '3K-10K', '>10K']\n",
    "        \n",
    "        success_rates = {sampler: [0]*len(size_labels) for sampler in sampler_names}\n",
    "        counts = [0] * len(size_labels)\n",
    "        \n",
    "        for result in results:\n",
    "            nodes = result['nodes']\n",
    "            size_idx = next(i for i, bin_max in enumerate(size_bins[1:]) if nodes <= bin_max)\n",
    "            counts[size_idx] += 1\n",
    "            \n",
    "            for sampler_name in sampler_names:\n",
    "                if result['samplers'][sampler_name]['successful_runs'] > 0:\n",
    "                    success_rates[sampler_name][size_idx] += 1\n",
    "        \n",
    "        # Normalize to percentages\n",
    "        for sampler_name in sampler_names:\n",
    "            for i in range(len(size_labels)):\n",
    "                if counts[i] > 0:\n",
    "                    success_rates[sampler_name][i] = 100 * success_rates[sampler_name][i] / counts[i]\n",
    "        \n",
    "        x = np.arange(len(size_labels))\n",
    "        width = 0.8 / len(sampler_names)\n",
    "        \n",
    "        for i, sampler_name in enumerate(sampler_names):\n",
    "            ax3.bar(x + i * width, success_rates[sampler_name], width, \n",
    "                   label=sampler_name, color=colors[i], alpha=0.7)\n",
    "        \n",
    "        ax3.set_xlabel('Problem Size')\n",
    "        ax3.set_ylabel('Success Rate (%)')\n",
    "        ax3.set_title('Success Rate by Problem Size')\n",
    "        ax3.set_xticks(x + width * (len(sampler_names) - 1) / 2)\n",
    "        ax3.set_xticklabels(size_labels)\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Best solutions found vs known optima\n",
    "        for result in results:\n",
    "            if result['best_known']:\n",
    "                best_found = max(\n",
    "                    sampler_result['best_cut'] \n",
    "                    for sampler_result in result['samplers'].values()\n",
    "                    if sampler_result['successful_runs'] > 0\n",
    "                )\n",
    "                ax4.scatter(result['best_known'], best_found, \n",
    "                           s=50, alpha=0.7, c=result['nodes'], cmap='viridis')\n",
    "        \n",
    "        # Perfect correlation line\n",
    "        if results:\n",
    "            all_known = [r['best_known'] for r in results if r['best_known']]\n",
    "            if all_known:\n",
    "                min_val, max_val = min(all_known), max(all_known)\n",
    "                ax4.plot([min_val, max_val], [min_val, max_val], \n",
    "                        'r--', alpha=0.5, label='Perfect')\n",
    "        \n",
    "        ax4.set_xlabel('Known Optimal Value')\n",
    "        ax4.set_ylabel('Best Found Value')\n",
    "        ax4.set_title('Solution Quality vs Known Optima')\n",
    "        cbar = plt.colorbar(ax4.collections[0] if ax4.collections else None, ax=ax4)\n",
    "        cbar.set_label('Number of Nodes')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plots\n",
    "        plot_file = self.results_dir / \"benchmark_plots.png\"\n",
    "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plots saved to: {plot_file}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4249b362-9cc6-48b7-a534-a1f9a2df3d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Max-Cut GPU Benchmarking with D-Wave Ocean SDK ===\n",
      "\n",
      "D-Wave sampler initialization failed: API token not defined\n",
      "Initialized samplers: ['neal', 'simulated_annealing', 'tabu']\n",
      "Available test configurations:\n",
      "1. Quick test: [11, 12]\n",
      "2. Full test: [11, 12, 13, 14, 43, 44, 48, 49, 54]\n",
      "3. Comprehensive test: [11, 12, 13, 14, 43, 44, 48, 49, 54, 70, 71]\n",
      "\n",
      "Running benchmark on instances: [11, 12]\n",
      "Starting systematic benchmark of 2 instances\n",
      "Instance progression: [11, 12]\n",
      "\n",
      "=== Benchmarking G11 ===\n",
      "Downloading G11...\n",
      "Nodes: 800, Edges: 1600\n",
      "QUBO terms: 2400, Best known: 564\n",
      "\n",
      "Testing neal...\n",
      "  Run 1: Cut=-530.0, Time=147.36s\n",
      "  Run 2: Cut=-530.0, Time=151.00s\n",
      "\n",
      "Benchmark interrupted by user\n",
      "\n",
      "=== Benchmark Complete ===\n",
      "Results saved in: maxcut_results\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main benchmarking script\"\"\"\n",
    "    print(\"=== Max-Cut GPU Benchmarking with D-Wave Ocean SDK ===\\n\")\n",
    "    \n",
    "    # Initialize benchmark system\n",
    "    benchmark = MaxCutBenchmark()\n",
    "    \n",
    "    # Define test progression\n",
    "    small_instances = [11, 12, 13, 14]  # Good for D-Wave testing\n",
    "    medium_instances = [43, 44]         # Borderline D-Wave capability\n",
    "    large_instances = [48, 49, 54]      # GPU territory\n",
    "    very_large_instances = [70, 71]     # Large-scale GPU testing\n",
    "    \n",
    "    # Choose your test set based on available time/resources\n",
    "    quick_test = small_instances[:2]\n",
    "    full_test = small_instances + medium_instances + large_instances\n",
    "    comprehensive_test = small_instances + medium_instances + large_instances + very_large_instances\n",
    "    \n",
    "    print(\"Available test configurations:\")\n",
    "    print(f\"1. Quick test: {quick_test}\")\n",
    "    print(f\"2. Full test: {full_test}\")\n",
    "    print(f\"3. Comprehensive test: {comprehensive_test}\")\n",
    "    \n",
    "    # Run benchmark (change this to your desired test set)\n",
    "    test_instances = quick_test  # Start with quick test\n",
    "    \n",
    "    print(f\"\\nRunning benchmark on instances: {test_instances}\")\n",
    "    results = benchmark.run_systematic_benchmark(test_instances, num_runs=3)\n",
    "    \n",
    "    print(\"\\n=== Benchmark Complete ===\")\n",
    "    print(f\"Results saved in: {benchmark.results_dir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d7a39-7e6d-4655-a6ee-385defb27169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
